{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18789bfb-889b-4032-a870-8b2d6d0121d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/conda/lib/\n"
     ]
    }
   ],
   "source": [
    "%env LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/conda/lib/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-german",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install bitsandbytes\n",
    "!pip install -q datasets loralib sentencepiece\n",
    "!pip install -q git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q git+https://github.com/huggingface/peft.git@64f63a7df2a02cfd144592d9aa9c871b59258c55\n",
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "arabic-defense",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda116.so\n",
      "CUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.9\n",
      "CUDA SETUP: Detected CUDA version 116\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda116.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('$LD_LIBRARY_PATH')}\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "BASE_MODEL = \"bigscience/bloomz-7b1-mt\"\n",
    "LORA_WEIGHTS = \"./bloomz-doctor\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map = {'': 0},\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, LORA_WEIGHTS, device_map={'': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "revised-registration",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_prompt(instruction):\n",
    "        return f\"\"\"Nếu bạn là bác sĩ, vui lòng trả lời các câu hỏi y tế dựa trên mô tả của bệnh nhân.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "agreed-maria",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(q):\n",
    "    input_ids = tokenizer(make_prompt(q), return_tensors=\"pt\")[\"input_ids\"].to('cuda')\n",
    "    with torch.no_grad():\n",
    "        gen_tokens = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens = 256,\n",
    "            do_sample=True,\n",
    "            temperature=0.5,\n",
    "            top_k=20,\n",
    "            repetition_penalty=1.2,\n",
    "            #eos_token_id=0, # for open-end generation.\n",
    "            #pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    origin_output = tokenizer.batch_decode(gen_tokens)[0]\n",
    "    output = origin_output.split(\"### Response:\")[1].strip()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "laden-chair",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://7a2a601727f271d76f.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://7a2a601727f271d76f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title GUI\n",
    "import gradio as gr\n",
    "examples = [\n",
    "    [\"Thưa bác sĩ, tôi bị nôn, buồn nôn và đau bụng. Gần đây tôi phát hiện ra rằng tôi có môn vị hẹp. Tôi nên dùng thuốc gì cho nó?\"],\n",
    "    [\"Chào bác sĩ, dạo gần đây tóc tôi bị rụng rất nhanh vài tháng vừa rồi. Tôi nghĩ rằng tôi có thể bị hội chứng rụng tóc. Tôi nên làm gì?\"],\n",
    "    [\"Thưa bác sĩ, tôi đã bị giật và co giật đột ngột trong cơ bắp của tôi. Nó trở nên rất khó quản lý chúng hàng ngày?\"],\n",
    "    [\"Tôi nghĩ mình bị ngộ độc carbon monoxide. Tôi cảm thấy chóng mặt và buồn nôn.\"],\n",
    "    [\"Tôi đã gặp vấn đề về trí nhớ và nhầm lẫn gần đây. Tôi nghĩ mình có thể mắc hội chứng Wernicke Korsakoff.\"],    \n",
    "]\n",
    "\n",
    "title = \"Bác sĩ Chat\"\n",
    "\n",
    "\n",
    "def inference(instruction):\n",
    "  return evaluate(instruction)\n",
    "\n",
    "io = gr.Interface(\n",
    "  inference,\n",
    "  inputs=[gr.Textbox(lines=5)],\n",
    "  outputs=[\n",
    "    gr.Textbox(lines=5, label=\"Bác sĩ Chat\")\n",
    "  ],\n",
    "  title=title,\n",
    "  examples=examples\n",
    ")\n",
    "io.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-farming",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-sheep",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
